# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e80QwdQjm0aprHoR8kuSQuY93gPKj6jW

# RAG Challenge - Retrieval Augmented Generation

## Step 1: Import necessary libraries and load the dataset
"""

import pandas as pd
import numpy as np
import os
import kagglehub

### Load Dataset: Download latest version
path = kagglehub.dataset_download("snap/amazon-fine-food-reviews")

# Load dataset from downloaded path
dataset_path = f"{path}/Reviews.csv"
df = pd.read_csv(dataset_path, nrows=1000)  # Load a small subset of 1000 rows

# Output preview of dataset
df.head()

"""## 2. Exploratory Data Analysis (EDA)"""

# General overview of dataset
df.info()

# Distribution of reviews by score
import matplotlib.pyplot as plt

df['Score'].value_counts().plot(kind='bar', title='Review Distribution by Score')
plt.show()

# Check for missing values
missing_values = df.isnull().sum()
print(f"Missing Values: \n{missing_values}")

# Understanding review length
df['review_length'] = df['Text'].apply(len)
df['review_length'].describe()

# Plot histogram of review length
df['review_length'].plot(kind='hist', title='Review Length Distribution')
plt.show()

"""## 3. Embedding and Storing Chunks"""

from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split

# Load embedding model
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Split dataset into chunks (e.g., reviews)
chunks = df['Text'].tolist()

# Generate embeddings for each chunk
embeddings = embedding_model.encode(chunks)

# Output embedding size for first review
print("Embedding size:", len(embeddings[0]))

"""## 4. Connecting to Vector Database"""

import chromadb

# Connect to ChromaDB client
chroma_client = chromadb.PersistentClient()

# Define the collection name
collection_name = 'product_review_embeddings'

# Create a collection in ChromaDB to store embeddings, if it doesn't already exist
try:
    collection = chroma_client.create_collection(name=collection_name)
except chromadb.errors.UniqueConstraintError:
    collection = chroma_client.get_collection(name=collection_name)

# Add embeddings to the collection
for i, embedding in enumerate(embeddings):
    collection.add(embeddings=[embedding.tolist()], metadatas=[{'content': chunks[i]}], ids=[str(i)])

"""## 5. Document Retrieval"""

# Test the retrieval logic
def retrieve_document(query, top_k=3):
    query_embedding = embedding_model.encode([query])
    results = collection.query(query_embeddings=query_embedding, n_results=top_k)
    return results

# Example query
query = "What do people think about the taste of this product?"
retrieved_docs = retrieve_document(query)
print("Top Retrieved Documents:\n", retrieved_docs)

"""## 6. Connecting to LLM"""

import openai
import pandas as pd
import numpy as np
import kagglehub
from sentence_transformers import SentenceTransformer
import chromadb

# Set up OpenAI API key
openai.api_key = "my-openai-api-key"
# 1. Dataset Selection
# Load Dataset using KaggleHub
path = kagglehub.dataset_download("snap/amazon-fine-food-reviews")
dataset_path = f"{path}/Reviews.csv"
df = pd.read_csv(dataset_path, nrows=1000)

# 2. Load Embedding Model
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# 3. Connect to ChromaDB and store embeddings
chroma_client = chroma_client = chromadb.PersistentClient()
collection_name = 'product_review_embeddings'

try:
    collection = chroma_client.create_collection(name=collection_name)
except chromadb.errors.UniqueConstraintError:
    collection = chroma_client.get_collection(name=collection_name)

chunks = df['Text'].tolist()
embeddings = embedding_model.encode(chunks)

for i, embedding in enumerate(embeddings):
    collection.add(embeddings=[embedding.tolist()], metadatas=[{'content': chunks[i]}], ids=[str(i)])

# Define a sample query
query = "What do people think about the taste of this product?"

# Function to retrieve documents
def retrieve_document(query, top_k=3):
    query_embedding = embedding_model.encode([query])
    results = collection.query(query_embeddings=query_embedding, n_results=top_k)
    return results

# Retrieve relevant documents
retrieved_docs = retrieve_document(query)

# Develop logic to generate response based on retrieved documents
def generate_response(query, retrieved_docs):
    combined_input = f"Question: {query}\n\nRelevant Documents:\n"
    for doc_list in retrieved_docs['metadatas']:
        for metadata in doc_list:
            combined_input += f"- {metadata['content']}\n"

    # Generate response using the OpenAI Chat API
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": combined_input}
        ],
        max_tokens=100
    )
    return response['choices'][0]['message']['content']

# Example response generation
response = generate_response(query, retrieved_docs)
print(f"Generated Response: {response}")

"""## 7. Evaluation"""

# Create test set of queries
queries = [
    "What do customers say about the quality of the product?",
    "Are people happy with the price of this item?",
    "What are the complaints regarding the packaging?"
]

# Evaluate system performance manually
for q in queries:
    docs = retrieve_document(q)
    resp = generate_response(q, docs)
    print(f"Query: {q}\nGenerated Response: {resp}\n")

"""## 8. Conclusion

Successfully implemented a RAG system using embeddings, ChromaDB, and OpenAI API. The system retrieves relevant information and generates responses to user queries.

**Future Improvements:**
- Handle longer and more complex queries effectively.
- Experiment with multimodal datasets.
"""